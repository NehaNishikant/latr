{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["!wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip\n","!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_train.json\n","!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_train.json\n","    \n","!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_val.json\n","!wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_val.json"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["!unzip './train_val_images.zip'"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Downloading and Installing the Libraries ‚öôÔ∏è:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ! pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-03T07:07:43.22341Z","iopub.status.busy":"2022-07-03T07:07:43.223171Z","iopub.status.idle":"2022-07-03T07:07:43.99082Z","shell.execute_reply":"2022-07-03T07:07:43.989874Z","shell.execute_reply.started":"2022-07-03T07:07:43.223383Z"},"id":"nyGaSE8uyaCq","outputId":"ac618563-c130-4362-a6cd-31334eabf9db","trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('src/latr/')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-03T07:05:32.819586Z","iopub.status.busy":"2022-07-03T07:05:32.819314Z","iopub.status.idle":"2022-07-03T07:05:34.501013Z","shell.execute_reply":"2022-07-03T07:05:34.500302Z","shell.execute_reply.started":"2022-07-03T07:05:32.819556Z"},"trusted":true},"outputs":[],"source":["## Logging into wandb\n","\n","import wandb\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n","wandb.login(key=secret_value_0)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Some important Library imports üõ≥Ô∏è"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-03T07:05:35.449789Z","iopub.status.busy":"2022-07-03T07:05:35.448962Z","iopub.status.idle":"2022-07-03T07:05:36.912396Z","shell.execute_reply":"2022-07-03T07:05:36.911325Z","shell.execute_reply.started":"2022-07-03T07:05:35.449634Z"},"id":"-_gXYvr0y9TV","trusted":true},"outputs":[],"source":["## Default Library import\n","\n","import os\n","import json\n","import numpy as np\n","from tqdm.auto import tqdm\n","import pandas as pd\n","import json\n","\n","## For the purpose of displaying the progress of map function\n","tqdm.pandas()\n","\n","## Visualization libraries\n","import pytesseract\n","from PIL import Image, ImageDraw\n","\n","## Specific libraries of LaTr\n","import torch.nn as nn\n","from dataset import load_json_file, get_specific_file, resize_align_bbox, get_tokens_with_boxes, create_features\n","from modeling import LaTr_for_pretraining, LaTr_for_finetuning\n","from utils import convert_ans_to_token, convert_ques_to_token, rotate, convert_token_to_ques, convert_token_to_answer\n","\n","## Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","## Warm-up\n","import pytorch_warmup as warmup\n","\n","import pytorch_lightning as pl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKz4ygsuNv8B","trusted":true},"outputs":[],"source":["## Setting the hyperparameters as well as primary configurations\n","\n","PAD_TOKEN_BOX = [0, 0, 0, 0]\n","max_seq_len = 512\n","batch_size = 2\n","target_size = (500,384) ## Note that, ViT would make it 224x224 so :(\n","t5_model = \"t5-base\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ClorTCszREw","trusted":true},"outputs":[],"source":["## Appending the ocr and json path\n","\n","base_path = './'\n","train_ocr_json_path = os.path.join(base_path, 'TextVQA_Rosetta_OCR_v0.2_train.json')\n","train_json_path = os.path.join(base_path, 'TextVQA_0.5.1_train.json')\n","\n","val_ocr_json_path = os.path.join(base_path, 'TextVQA_Rosetta_OCR_v0.2_val.json')\n","val_json_path = os.path.join(base_path, 'TextVQA_0.5.1_val.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjXZNCoP0I0x","trusted":true},"outputs":[],"source":["## Loading the files\n","\n","train_ocr_json = json.load(open(train_ocr_json_path))['data']\n","train_json = json.load(open(train_json_path))['data']\n","\n","val_ocr_json = json.load(open(val_ocr_json_path))['data']\n","val_json = json.load(open(val_json_path))['data']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjD8348YTJSd","trusted":true},"outputs":[],"source":["## Useful for the key-value extraction\n","\n","train_json_df = pd.DataFrame(train_json)\n","train_ocr_json_df = pd.DataFrame(train_ocr_json)\n","\n","val_json_df = pd.DataFrame(val_json)\n","val_ocr_json_df = pd.DataFrame(val_ocr_json)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Du-EoByODjD","trusted":true},"outputs":[],"source":["## Converting list to string\n","\n","train_json_df['answers'] = train_json_df['answers'].apply(lambda x: \" \".join(list(map(str, x))))\n","val_json_df['answers']   = val_json_df['answers'].apply(lambda x: \" \".join(list(map(str, x))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-nEMDBNAVuy","outputId":"7de74e20-ed6f-4b6e-bb52-5a4bd041df5f","trusted":true},"outputs":[],"source":["## Dropping of the images which doesn't exist, might take some time\n","\n","base_img_path = os.path.join('./', 'train_images')\n","train_json_df['path_exists'] = train_json_df['image_id'].progress_apply(lambda x: os.path.exists(os.path.join(base_img_path, x)+'.jpg'))\n","train_json_df = train_json_df[train_json_df['path_exists']==True]\n","\n","val_json_df['path_exists'] = val_json_df['image_id'].progress_apply(lambda x: os.path.exists(os.path.join(base_img_path, x)+'.jpg'))\n","val_json_df = val_json_df[val_json_df['path_exists']==True]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lipkoibdUeFT","outputId":"0e937aa6-545a-4854-ee16-ea8242fdf230","trusted":true},"outputs":[],"source":["## Dropping the unused columns\n","\n","train_json_df.drop(columns = ['flickr_original_url', 'flickr_300k_url','image_classes', 'question_tokens', 'path_exists'\n","                              ], axis = 1, inplace = True)\n","val_json_df.drop(columns = ['flickr_original_url', 'flickr_300k_url','image_classes', 'question_tokens', 'path_exists'\n","                              ], axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_ABFoJsTpvd","trusted":true},"outputs":[],"source":["## Deleting the json\n","\n","del train_json\n","del train_ocr_json\n","del val_json\n","del val_ocr_json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeloNT7dT51I","trusted":true},"outputs":[],"source":["## Grouping for the purpose of feature extraction\n","grouped_df = train_json_df.groupby('image_id')\n","\n","## Getting all the unique keys of the group by object\n","keys = list(grouped_df.groups.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0YMTxHjfw1R","outputId":"933cb213-ee32-42ee-e7b7-8ed57d436bc0","trusted":true},"outputs":[],"source":["## Tokenizer import\n","\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","tokenizer = T5Tokenizer.from_pretrained(t5_model)"]},{"cell_type":"markdown","metadata":{"id":"8l8qH9ZYDFjh"},"source":["## 4. Making the dataset üíΩ:\n","The main idea behind making the dataset is, to pre-process the input into a given format, and then provide the input to the model. So, simply just the image path, and the other configurations, and boom üí•, you would get the desired pre-processed input"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sent = tokenizer.encode(\"what is the url in the picture?\", max_length = 512, truncation = True, padding = 'max_length', return_tensors = 'pt')[0]\n","# dec_sent = tokenizer.decode(sent, skip_special_tokens = True)\n","# dec_sent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9jE7KKRPFkg","trusted":true},"outputs":[],"source":["## Defining the pytorch dataset\n","\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from torchvision import transforms\n","\n","class TextVQA(Dataset):\n","  def __init__(self, base_img_path, json_df, ocr_json_df, tokenizer, transform = None, max_seq_length = 512, target_size = (500,384), fine_tune = True):\n","\n","    self.base_img_path = base_img_path\n","    self.json_df = json_df\n","    self.ocr_json_df = ocr_json_df\n","    self.tokenizer = tokenizer\n","    self.target_size = target_size\n","    self.transform = transform\n","    self.max_seq_length = max_seq_length\n","    self.fine_tune = fine_tune\n","\n","  def __len__(self):\n","    return len(self.json_df)\n","\n","  def __getitem__(self, idx):\n","\n","    curr_img = self.json_df.iloc[idx]['image_id']\n","    ocr_token = self.ocr_json_df[self.ocr_json_df['image_id']==curr_img]['ocr_info'].values.tolist()[0]\n","\n","    boxes = []\n","    words = []\n","\n","    current_group = self.json_df.iloc[idx]\n","    width, height = current_group['image_width'], current_group['image_height']\n","\n","    ## Getting the ocr and the corresponding bounding boxes\n","    for entry in ocr_token:\n","      xmin, ymin, w, h, angle = entry['bounding_box']['top_left_x'], entry['bounding_box']['top_left_y'],  entry['bounding_box']['width'],  entry['bounding_box']['height'], entry['bounding_box']['rotation']\n","      xmin, ymin,w, h = resize_align_bbox([xmin, ymin, w, h], 1, 1, width, height)\n","      \n","      x_centre = xmin + (w/2)\n","      y_centre = ymin + (h/2)\n","\n","      ## print(\"The angle is:\", angle)\n","      xmin, ymin = rotate([x_centre, y_centre], [xmin, ymin], angle)\n","\n","      xmax = xmin + w\n","      ymax = ymin + h\n","\n","      ## Bounding boxes are normalized\n","      curr_bbox = [xmin, ymin, xmax, ymax]\n","      boxes.append(curr_bbox)\n","      words.append(entry['word'])\n","\n","    img_path = os.path.join(self.base_img_path, curr_img)+'.jpg'  ## Adding .jpg at end of the image, as the grouped key does not have the extension format \n","\n","    assert os.path.exists(img_path)==True, f'Make sure that the image exists at {img_path}!!'\n","    ## Extracting the feature\n","    \n","    if self.fine_tune:\n","        \n","        ## For fine-tune stage, they use [0, 0, 1000, 1000] for all the bounding box\n","        img = Image.open(img_path).convert(\"RGB\")\n","        img = img.resize(self.target_size)\n","        boxes = torch.zeros(self.max_seq_length, 4)\n","        boxes[:, 2] = 1000\n","        boxes[:, 3] = 1000\n","        \n","        words = \" \".join(words)\n","        tokenized_words = self.tokenizer.encode(words, max_length = self.max_seq_length, truncation = True, padding = 'max_length', return_tensors = 'pt')[0]\n","        \n","    else:\n","        ## For pre-train, this strategy would be useful\n","        img, boxes, tokenized_words = create_features(image_path = img_path,\n","                                                      tokenizer = self.tokenizer,\n","                                                      target_size = self.target_size,\n","                                                      max_seq_length = self.max_seq_length,\n","                                                      use_ocr = False,\n","                                                      bounding_box = boxes,\n","                                                      words = words\n","                                                      )\n","    \n","    ## Converting the boxes as per the format required for model input\n","    boxes = torch.as_tensor(boxes, dtype=torch.int32)\n","    width = (boxes[:, 2] - boxes[:, 0]).view(-1, 1)\n","    height = (boxes[:, 3] - boxes[:, 1]).view(-1, 1)\n","    boxes = torch.cat([boxes, width, height], axis = -1)\n","\n","    ## Clamping the value,as some of the box values are out of bound\n","    boxes[:, 0] = torch.clamp(boxes[:, 0], min = 0, max = 1000)\n","    boxes[:, 2] = torch.clamp(boxes[:, 2], min = 0, max = 1000)\n","    boxes[:, 4] = torch.clamp(boxes[:, 4], min = 0, max = 1000)\n","    \n","    boxes[:, 1] = torch.clamp(boxes[:, 1], min = 0, max = 1000)\n","    boxes[:, 3] = torch.clamp(boxes[:, 3], min = 0, max = 1000)\n","    boxes[:, 5] = torch.clamp(boxes[:, 5], min = 0, max = 1000)\n","    \n","    ## Tensor tokenized words\n","    tokenized_words = torch.as_tensor(tokenized_words, dtype=torch.int32)\n","\n","    if self.transform is not None:\n","      img = self.transform(img)\n","    else:\n","      img = transforms.ToTensor()(img)\n","\n","\n","    ## Getting the Question\n","    question = current_group['question']   \n","    question = convert_ques_to_token(question = question, tokenizer = self.tokenizer)\n","\n","    ## Getting the Answer\n","    answer = current_group['answers']\n","    answer = convert_ques_to_token(question = answer, tokenizer = self.tokenizer).long()\n","\n","    return {'img':img, 'boxes': boxes, 'tokenized_words': tokenized_words, 'question': question, 'answer': answer, 'id': torch.as_tensor(idx)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybhCeKY2aSZd","trusted":true},"outputs":[],"source":["train_ds = TextVQA(base_img_path = base_img_path,\n","                   json_df = train_json_df,\n","                   ocr_json_df = train_ocr_json_df,\n","                   tokenizer = tokenizer,\n","                   transform = None, \n","                   max_seq_length = max_seq_len, \n","                   target_size = target_size\n","                   )\n","\n","\n","val_ds = TextVQA(base_img_path = base_img_path,\n","                   json_df = val_json_df,\n","                   ocr_json_df = val_ocr_json_df,\n","                   tokenizer = tokenizer,\n","                   transform = None, \n","                   max_seq_length = max_seq_len, \n","                   target_size = target_size\n","                   )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPpnuqANe6j8","trusted":true},"outputs":[],"source":["idx = 1569"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwbsVenve7n7","trusted":true},"outputs":[],"source":["encoding = train_ds[idx]  ## Might take time as per the processing\n","for key in list(encoding.keys()):\n","  print_statement = '{0: <30}'.format(str(key) + \" has a shape:\")\n","  print(print_statement, encoding[key].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGUXoIWkf9lP","trusted":true},"outputs":[],"source":["## Sample Img, Sample box, sample words, sample answer, sample question\n","\n","s_img = encoding['img']\n","s_boxes = encoding['boxes']\n","s_words = encoding['tokenized_words']\n","s_ans = encoding['answer']\n","s_ques = encoding['question']"]},{"cell_type":"markdown","metadata":{"id":"sFfIfM8_K_gX"},"source":["## Decoding the Question, Answer as well as the Image for the post-processing"]},{"cell_type":"markdown","metadata":{"id":"1DjQAqAfMvWA"},"source":["### 1. Image Part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFs-5XMigMD-","trusted":true},"outputs":[],"source":["## OCR and Image part\n","actual_img = transforms.ToPILImage()(s_img).convert(\"RGB\")\n","actual_boxes = s_boxes[:, :4].numpy().tolist()  ## (xmin, ymin, xmax, ymax)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEgQAAZOgdvU","trusted":true},"outputs":[],"source":["## If use fine_tune = False, we can get the ocr as well\n","\n","# from PIL import Image, ImageDraw\n","\n","# # create rectangle image\n","# draw_on_img = ImageDraw.Draw(actual_img)  \n","\n","# for box in actual_boxes:\n","#     draw_on_img.rectangle(box, outline =\"red\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYitD-Uwx5EY","outputId":"13e57e2b-767d-4f91-9140-31f22734debf","trusted":true},"outputs":[],"source":["actual_img"]},{"cell_type":"markdown","metadata":{"id":"sSBYRkXdMy-o"},"source":["### 2. Question Part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZMwxyHMMW3r","outputId":"d89de636-ddfb-486d-8b08-fb099c27df7d","trusted":true},"outputs":[],"source":["decoded_ques = convert_token_to_ques(s_ques, tokenizer)\n","print(decoded_ques)  ## Goes well!!"]},{"cell_type":"markdown","metadata":{"id":"e19RH2OfM1ID"},"source":["### 3. Answer Part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rBNtJJeO8IQ","outputId":"b3f74ba6-5624-4c14-c65f-8a4ff8cfe882","trusted":true},"outputs":[],"source":["decoded_answer = convert_token_to_ques(s_ans, tokenizer)\n","print(decoded_answer)  ## Goes Well!!"]},{"cell_type":"markdown","metadata":{"id":"nRNEhRLIUiY_"},"source":["## 4.2. Making the Collate function for DataLoader:\n","\n","More on collate function can be known from [here](https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-03T07:05:44.529726Z","iopub.status.busy":"2022-07-03T07:05:44.529388Z","iopub.status.idle":"2022-07-03T07:05:44.537709Z","shell.execute_reply":"2022-07-03T07:05:44.536722Z","shell.execute_reply.started":"2022-07-03T07:05:44.52969Z"},"id":"O2Kd4Gw52FEo","trusted":true},"outputs":[],"source":["from transformers import ViTFeatureExtractor, ViTModel\n","vit_feat_extract = ViTFeatureExtractor(\"google/vit-base-patch16-224-in21k\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWJX6VFFN6su","trusted":true},"outputs":[],"source":["def collate_fn(data_bunch):\n","\n","  '''\n","  A function for the dataloader to return a batch dict of given keys\n","\n","  data_bunch: List of dictionary\n","  '''\n","\n","  dict_data_bunch = {}\n","\n","  for i in data_bunch:\n","    for (key, value) in i.items():\n","      if key not in dict_data_bunch:\n","        dict_data_bunch[key] = []\n","      dict_data_bunch[key].append(value)\n","\n","  for key in list(dict_data_bunch.keys()):\n","      dict_data_bunch[key] = torch.stack(dict_data_bunch[key], axis = 0)\n","\n","  if 'img' in dict_data_bunch:\n","    ## Pre-processing for ViT\n","    dict_data_bunch['img'] = vit_feat_extract(list(dict_data_bunch['img']),return_tensors = 'pt')['pixel_values']\n","\n","  return dict_data_bunch"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Defining the DataModule üìñ\n","\n","* A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data:\n","\n","* A DataModule is simply a collection of a train_dataloader(s), val_dataloader(s), test_dataloader(s) and predict_dataloader(s) along with the matching transforms and data processing/downloads steps required.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9jqgGGkUkeO","trusted":true},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","\n","  def __init__(self, train_dataset, val_dataset,  batch_size = 1):\n","\n","    super(DataModule, self).__init__()\n","    self.train_dataset = train_dataset\n","    self.val_dataset = val_dataset\n","    self.batch_size = batch_size\n","\n","    \n","  ## The parameters for Dataloader have been taken from here: https://docs.mosaicml.com/en/v0.7.1/trainer/dataloaders.html#passing-a-pytorch-dataloader\n","  def train_dataloader(self):\n","    return DataLoader(self.train_dataset, batch_size = self.batch_size, \n","                      collate_fn = collate_fn, shuffle = True, num_workers = 2, pin_memory = True, persistent_workers = True)\n","  \n","  def val_dataloader(self):\n","    return DataLoader(self.val_dataset, batch_size = self.batch_size,\n","                    collate_fn = collate_fn, shuffle = False, num_workers = 2, pin_memory = True, persistent_workers = True)"]},{"cell_type":"markdown","metadata":{"id":"rFQUQOr7w_9q"},"source":["## 6. Modeling Part üèéÔ∏è\n","1. Firstly, we would define the pytorch model with our configurations\n","2. Secondly, we would encode it in the PyTorch Lightening module, and boom üí• our work of defining the model is done"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-btmDfw-UxNZ","trusted":true},"outputs":[],"source":["## keys are img, boxes, tokenized_words, answer, question"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kJ2kOk4yW1E","trusted":true},"outputs":[],"source":["config = {\n","    't5_model': 't5-base',\n","    'vocab_size': 32128,\n","    'hidden_state': 768,\n","    'max_2d_position_embeddings': 1001,\n","    'classes': 2,  ## yes/no sarcastic\n","    'seq_len': 512\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEp9Bm2D3-SM","trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sa0JrVmp0_Od","trusted":true},"outputs":[],"source":["# finetuned_latr = LaTr_for_finetuning(config).to(device)\n","# datamodule = DataModule(train_ds, val_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","def calculate_acc_score(pred, gt):\n","    \n","    ## Function ignores the calculation of padding part\n","    ## Shape (seq_len, seq_len)\n","    mask = torch.clamp(gt, min = 0, max = 1)\n","    last_non_zero_argument = (mask != 0).nonzero()[1][-1]\n","    pred = pred[:last_non_zero_argument]\n","    gt = gt[:last_non_zero_argument]  ## Include all the arguments till the first padding index\n","    \n","    return accuracy_score(pred, gt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtvisUqvmSz3","trusted":true},"outputs":[],"source":["## https://stackoverflow.com/questions/69899602/linear-decay-as-learning-rate-scheduler-pytorch\n","def polynomial(base_lr, iter, max_iter = 1e5, power = 1):\n","    return base_lr * ((1 - float(iter) / max_iter) ** power)\n","\n","class LaTrForVQA(pl.LightningModule):\n","  def __init__(self, config , learning_rate = 1e-4, max_steps = 100000//2, address_to_pre_trained_weights=None):\n","    super(LaTrForVQA, self).__init__()\n","    \n","    self.config = config\n","    self.save_hyperparameters()\n","    self.latr =  LaTr_for_finetuning(config, address_to_pre_trained_weights=address_to_pre_trained_weights)\n","    self.training_losses = []\n","    self.validation_losses = []\n","    self.max_steps = max_steps\n","\n","#   def configure_optimizers(self):\n","#     optimizer = torch.optim.AdamW(self.parameters(), lr = self.hparams['learning_rate'])\n","#     warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = 1000)  \n","#     scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,total_iters  = self.max_steps,  verbose = True)\n","#     return [optimizer], [{\"scheduler\": (lr_scheduler, warmup_scheduler), \"interval\": \"step\"}]\n","\n","#   def lr_scheduler_step(self, scheduler, optimizer_idx, metric):\n","#         lr_scheduler, warmup_scheduler = scheduler\n","#         with warmup_scheduler.dampening():\n","#                 lr_scheduler.step()\n","\n","  def configure_optimizers(self):\n","    return torch.optim.AdamW(self.parameters(), lr = self.hparams['learning_rate'])\n","\n","\n","  def forward(self, batch_dict):\n","    boxes =   batch_dict['boxes']\n","    img =     batch_dict['img']\n","    question = batch_dict['question']\n","    words =   batch_dict['tokenized_words']\n","    answer_vector = self.latr(lang_vect = words, \n","                               spatial_vect = boxes, \n","                               img_vect = img, \n","                               quest_vect = question\n","                               )\n","    return answer_vector\n","\n","  def calculate_metrics(self, prediction, labels):\n","\n","      ## Calculate the accuracy score between the prediction and ground label for a batch, with considering the pad sequence\n","      batch_size = len(prediction)\n","      ac_score = 0\n","\n","      for (pred, gt) in zip(prediction, labels):\n","        ac_score+= calculate_acc_score(pred.detach().cpu(), gt.detach().cpu())\n","      ac_score = ac_score/batch_size\n","      return ac_score\n","\n","  def training_step(self, batch, batch_idx):\n","    answer_vector = self.forward(batch)\n","\n","    ## https://discuss.huggingface.co/t/bertformaskedlm-s-loss-and-scores-how-the-loss-is-computed/607/2\n","    loss = nn.CrossEntropyLoss()(answer_vector.reshape(-1,self.config['classes']), batch['answer'].reshape(-1))\n","    _, preds = torch.max(answer_vector, dim = -1)\n","\n","    ## Calculating the accuracy score\n","    train_acc = self.calculate_metrics(preds, batch['answer'])\n","    train_acc = torch.tensor(train_acc)\n","\n","    ## Logging\n","    self.log('train_ce_loss', loss,prog_bar = True)\n","    self.log('train_acc', train_acc, prog_bar = True)\n","    self.training_losses.append(loss.item())\n","\n","    return loss\n","\n","  def validation_step(self, batch, batch_idx):\n","    logits = self.forward(batch)\n","    loss = nn.CrossEntropyLoss()(logits.reshape(-1,self.config['classes']), batch['answer'].reshape(-1))\n","    _, preds = torch.max(logits, dim = -1)\n","\n","    ## Validation Accuracy\n","    val_acc = self.calculate_metrics(preds.cpu(), batch['answer'].cpu())\n","    val_acc = torch.tensor(val_acc)\n","\n","    ## Logging\n","    self.log('val_ce_loss', loss, prog_bar = True)\n","    self.log('val_acc', val_acc, prog_bar = True)\n","    \n","    return {'val_loss': loss, 'val_acc': val_acc}\n","  ## For the fine-tuning stage, Warm-up period is set to 1,000 steps and again is linearly decayed to zero, pg. 12, of the paper\n","  ## Refer here: https://github.com/Lightning-AI/lightning/issues/328#issuecomment-550114178\n","  \n","  def optimizer_step(self, epoch_nb, batch_nb, optimizer, optimizer_i, opt_closure = None, on_tpu=False,\n","    using_native_amp=False, using_lbfgs=False):\n","\n","        ## Warmup for 1000 steps\n","        if self.trainer.global_step < 1000:\n","            lr_scale = min(1., float(self.trainer.global_step + 1) / 1000.)\n","            for pg in optimizer.param_groups:\n","                pg['lr'] = lr_scale * self.hparams.learning_rate\n","\n","        ## Linear Decay\n","        else:\n","            for pg in optimizer.param_groups:\n","                pg['lr'] = polynomial(self.hparams.learning_rate, self.trainer.global_step, max_iter = self.max_steps)\n","\n","        optimizer.step(opt_closure)\n","        optimizer.zero_grad()\n","\n","  def validation_epoch_end(self, outputs):\n","        \n","        \n","        val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n","\n","        self.log('val_loss_epoch_end', val_loss, on_epoch=True, sync_dist=True)\n","        self.log('val_acc_epoch_end', val_acc, on_epoch=True, sync_dist=True)\n","        \n","        self.val_prediction = []\n","        \n","#   def training_epoch_end(self, training_step_outputs):\n","#     train_loss_mean = np.mean(self.training_losses)\n","#     self.logger.experiment.add_scalar('training_loss', train_loss_mean, global_step=self.current_epoch)\n","#     self.training_losses = []  # reset for next epoch\n","\n","#   def validation_epoch_end(self, validation_step_outputs):\n","#     val_loss_mean = np.mean(self.training_losses)\n","#     self.logger.experiment.add_scalar('validation_loss', val_loss_mean, global_step=self.current_epoch)\n","#     self.validation_losses = []  # reset for next epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["url_for_ckpt = 'https://www.kaggleusercontent.com/kf/99663112/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..GfuZWkqwWi9nROCTnAS3OQ.YowTb3CNlES2WS_F6BvOSrGs3uLWc2kSBkhElYUcndML0Feuiizdu8trA2e4aj_kdluv1nYlVpS3_86VaJfgSBtyJShQoB0CyxCqdvdMiKl4eQQdWUv2XrTBecEJPXupdFaElzr57CcRjpz35rueyDjf3GVJLznkpSdoyWwSxoxCACbUpS73PKWi97WHfPmEWQgXTDxT_Uno_Pau6fayKyzJ-vWrETzOA2Z6f1-i7umK48D7JBQacS2g_40dW8wIH34QsztCZhHOake7qZnXU_19qaFeDQCNldZ4HcGAmKMtqYI_NK_By370IZ6OHe5Q-mh1f_9SaZoXCzzgaNx4Wsw1THZgzSjZgP2dTLP6a4ZkjHFWiZdkl0azvmoCmSVVYbRdQ9_iI9sFvhUpDWj1bOlr-Zrq9gRi8ksaH9rIzrzk63x_fKPGphZKpxB_l_6iewdGt4yb3GB8kWyGrxBnsGvV5Ei7gTaqv9OAkSKTACMEKB-rj-T8HKtk3ktnEqGMCpHTpkB8RYE6EqYRPbnSYMShjZb12GSn5uYntLtcG7MUbQX-OMt0vzh9fag_zpCyO89K56jxZ6Q9kWdADG0C2T0nR8uC8vWUUBptWNc2tt6pcupcUO19kt7ddNHMbxajHym5AijizrfJbkqnujEodlHWc8C77PawpX2xUPvIlbSvhbdsRRyYfOFGLmZsDdKa.c9dgiKXE5w_-qo4J3He6Qw/models/epoch=0-step=34602.ckpt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dR_3-4qxuDIi","trusted":true},"outputs":[],"source":["from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import WandbLogger\n","\n","def main():\n","    datamodule = DataModule(train_ds, val_ds)\n","    max_steps = 50000       ## 60K Steps\n","    latr = LaTrForVQA(config, max_steps= max_steps, address_to_pre_trained_weights=\"models/pretrain.pt\")\n","    \n","    try:\n","        latr = latr.load_from_checkpoint(url_for_ckpt)\n","        print(\"Checkpoint loaded correctly\")\n","    except:\n","        print(\"Could not load checkpoint\")\n","        return \n","    \n","    checkpoint_callback = ModelCheckpoint(\n","        dirpath=\"./models\", monitor=\"val_ce_loss\", mode=\"min\"\n","    )\n","    \n","    wandb.init(config=config, project=\"VQA with LaTr\")\n","    wandb_logger = WandbLogger(project=\"VQA with LaTr\", log_model = True, entity=\"iakarshu\")\n","    \n","    ## https://www.tutorialexample.com/implement-reproducibility-in-pytorch-lightning-pytorch-lightning-tutorial/\n","    pl.seed_everything(42, workers=True)\n","    \n","    trainer = pl.Trainer(\n","        max_steps = max_steps,\n","        default_root_dir=\"logs\",\n","        gpus=(1 if torch.cuda.is_available() else 0),\n","#         accelerator=\"tpu\",\n","#         devices=8,\n","        logger=wandb_logger,\n","        callbacks=[checkpoint_callback],\n","        deterministic=True\n","    )\n","    \n","    trainer.fit(latr, datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sample = LaTr_for_finetuning(config).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Debug\n","\n","# for batch in tqdm(DataModule(train_ds, val_ds).train_dataloader(), total = len(DataModule(train_ds, val_ds).train_dataloader())):\n","#     try:\n","#         out = sample.pre_training_model.top_left_y(batch['boxes'][:, :, 1])\n","#         out = sample.pre_training_model.top_left_x(batch['boxes'][:, :, 0])\n","#         out = sample.pre_training_model.bottom_right_x(batch['boxes'][:, :, 2])\n","#         out = sample.pre_training_model.bottom_right_y(batch['boxes'][:, :, 3])\n","#         out = sample.pre_training_model.width_emb(batch['boxes'][:, :, 4])\n","#         out = sample.pre_training_model.height_emb(batch['boxes'][:, :, 5])\n","#     except:\n","#         print(\"Problem found....\")\n","#         break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjzjeQ4TuilG","trusted":true},"outputs":[],"source":["if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{},"source":["## References:\n","\n","1. [MLOps Repo](https://github.com/graviraja/MLOps-Basics) (For the integration of model and data with PyTorch Lightening) \n","2. [PyTorch Lightening Docs](https://pytorch-lightning.readthedocs.io/en/stable/index.html) For all the doubts and bugs\n","3. [My Repo](https://github.com/uakarsh/latr) For downloading the model and pre-processing steps\n","4. Google for other stuffs"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.15 ('latr')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.15"},"vscode":{"interpreter":{"hash":"32b36d420d42201d5c79536b64b6d65f24716803f330c6bd6b2374491726117b"}}},"nbformat":4,"nbformat_minor":4}
